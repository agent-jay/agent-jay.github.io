---
title: Sequence to Sequence Learning Meta-post
toc: true
---

I've studied neural nets before in classes but my first serious foray into
modern "deep learning" architectures is through Sequence-to-Sequence models. Suffice to
say most of what I learnt was new to me. Here I'm going to lay out the resources that 
I wish I found when I first got started.  

## Background

If you're like me, you might have studied the basics of neural nets, backpropagation and
Stochastic Gradient Descent at the level covered by Andrew Ng in his ML class. Modern Deep Learning architectures are far more complex especially when you're looking to implement them. 

- [Introduction to RNNs](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)
    - a four part series that covers simple RNNs, Backpropagation Through Time (BPTT)
    - BPTT is needed to perform backpropagation for RNNs. Straightforward application of the chain rule, but over a single sequential training sample
- [Introduction to LSTMs](colah.github.io/posts/2015-08-Understanding-LSTMs/)
    - great introduction to LSTMs by Cristopher Olah. Imo, you don't need to know too much about LSTMs when getting started, or even when you start coding. Treat them as a modular unit that are functionally identical to RNNs.
 
## Sequence to Sequence models

- [Sequence to Sequence Learning](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)
    - The most commonly used encoder-decoder style architecture used. Read Quoc Le's tutorial (listed in implementation)
- [Learning to Jointly Align and Translate](https://arxiv.org/abs/1409.0473)
    - The first paper you should read about attention. Assumes you're already familiar with RNNs, LSTMs and encoder-decoder sty

## Implementation

### Keras

I played around with Keras last week. It has a dead simple API that you
definitely want if your task is fairly standard. Keras does have a good RNN
module and can be used for tasks like sentiment detection, language modeling,
NER. But the abstraction seems insufficient for Sequence to Sequence models,
and I couldn't find any non-toy implementations using Keras.  Tensorflow seems
like the defacto 

Rule of thumb seems to be that the more standard a technique is, the more
likely that Keras has assimilated it.

### Tensorflow

 - Excellent [notebooks](https://github.com/ematvey/tensorflow-seq2seq-tutorials) on Sequence-to-Sequence models in Tensorflow. 
    - Consolidates a lot of details needed when building practical seq2seq models such as padding, dynamic batch/sequence sizes (dynamic_rnn API), Bi-directional RNNs, and raw_rnn TF API needed for implementing more complex decoders (eg. attention models). Missing some details such as how to implement beam search and masking the loss when working with variable output lengths
 - [Tutorial on RNNs by Quoc Le](http://cs.stanford.edu/~quocle/tutorial2.pdf)
    - Elaborates on some of the tricks of the trade for sequence models that are glossed over in the original paper, especially about the decoder (greedy search, beam search, etc.)
 - [Stackexchange thread on the same topic](https://cs.stackexchange.com/questions/69432/whats-the-input-to-the-decoder-in-a-sequence-to-sequence-autoencoder)
 - Denny Britz brings it again with a great list of [best practices for working with RNNs in Tensorflow](http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/)
    - This was the first time I heard of a method to compute accurate losses for variable length outputs
    - Seriously. Read it all

### Tensorflow basics, nuances
 - [Variable sharing, namespaces](http://web.stanford.edu/class/cs224n/lecture_notes/cs224n-2017-tensorflow.pdf)
    - I had some trouble understanding the point of the get_variable and variable_scope functions until I read these. Lecture notes from the cs224n Stanford class.
 - [TF Dev Summit '17 primer to the new RNN and seq2seq API](https://www.youtube.com/watch?v=RIR_-Xlbp7s&feature=youtu.be)
